# Ethical Vendor Benchmarking Manifesto

Vendor Benchmarking of a competing product in the market place is a common form of marketing. However such “Bench Marketing” often leads to accusations of unfairness and unprofessionalism by the competing party, when their product is shown in a less favorable light than that of the publisher of the benchmark.

Manipulation can often be quite subtle and not obvious to the reader, especially if the benchmark is not reproducible or the source code is not available. Unreputable and deliberately misleading benchmarks can do considerable damage to a competing party.

This Manifesto is an attempt to provide ethical ground rules for vendors publishing benchmarks against competitor’s products. Vendors wishing to abide by this Manifesto are free to label their benchmark as an Ethical Vendor Benchmark.

This Manifesto is a living document, it is Open Source and contributions from the community are welcome in the form of Issues or Pull Requests submited via the github project.

We invite software vendors to sign up to this manifesto and will display signatories on an attached page.

The six points of the manifesto are as follows:

## Reproducible
Any third party should have the ability to run an ethical benchmark. This includes making the source code of the benchmarks available along with concise instructions on how to build, install and execute the benchmarks.  If any part of the benchmark requires a license to run proprietary software, this should be available via a trial to all participants.

## Transparent
The source code of the benchmark should be publicly available (for example on Github) and the repository should accept relevant pull requests from any party in a timely manner. If vendor benchmarks are more open, they may benefit the community at large as a starting point for their own investigations.

## Equalized
The author of the benchmark should seek in good faith to equalize as many components of the benchmark as possible such as Operating System, Client Languages, Benchmarking Tools, etc. An example of an unethical benchmark would be comparing two systems where one benchmark uses Language A with Benchmarking Tool B, whilst the other side uses Language C with Benchmarking Tool D. The publisher will be expected to research the competing product sufficiently to use the fairest API and configurations.

## Sensible
A benchmark should always seek to reproduce a “sensible scenario” that a user might reasonably run in a production environment.

## Fair Warning
The author of the benchmark should take reasonable means to contact the competing party at least 1 month prior to publication of the benchmark to customers or public forums. The author will fairly assist the competing party in line with “Reproducible Benchmarks” and should allow the competing party to provide changes to the source code of the benchmark affecting the competing party’s results.

## Right of Reply
The benchmark publication should include a right of reply whereby the competing party can provide a response, often as a conclusion to the “Fair Warning” process. In practice, this should be a link to a relevant publication produced by the competing party. The publisher should not be expected to include a lengthy rebuttal.
